"""
FastAPI ä¸»å…¥å£ä¸è·¯ç”±å®šä¹‰
"""
import logging
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse
from contextlib import asynccontextmanager

from app.config import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶è¾“å‡ºé…ç½®è­¦å‘Š
    config.log_startup_warnings()
    logger.info("ğŸš€ API Relay å¯åŠ¨å®Œæˆ")
    logger.info(f"   - OpenAI ä¸Šæ¸¸: {config.UPSTREAM_OPENAI_BASE_URL}")
    logger.info(f"   - Gemini ä¸Šæ¸¸: {config.UPSTREAM_GEMINI_BASE_URL}")
    logger.info(f"   - Claude ä¸Šæ¸¸: {config.UPSTREAM_CLAUDE_BASE_URL}")
    logger.info(f"   - æŠ—æˆªæ–­: {'é»˜è®¤å¯ç”¨' if config.ANTI_TRUNCATION_ENABLED_DEFAULT else 'é»˜è®¤ç¦ç”¨'}")
    yield
    logger.info("ğŸ‘‹ API Relay å…³é—­")


app = FastAPI(
    title="LLM API Relay with Anti-Truncation",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "service": "llm-api-relay",
        "version": "1.0.0",
        "features": ["openai", "gemini", "claude", "anti-truncation", "transparent-proxy"]
    }


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy"}


# ==================== OpenAI Compatible API ====================

@app.post("/v1/chat/completions")
async def openai_chat_completions(request: Request):
    """OpenAI Chat Completions API"""
    from app.routes import handle_openai_chat_completions
    return await handle_openai_chat_completions(request)


# ==================== Gemini API ====================

@app.post("/v1/models/{model}:generateContent")
async def gemini_generate_content(request: Request, model: str):
    """Gemini generateContent APIï¼ˆéæµå¼ï¼‰"""
    from app.routes import handle_gemini_generate_content
    return await handle_gemini_generate_content(request, model, is_streaming=False, is_beta=False)


@app.post("/v1/models/{model}:streamGenerateContent")
async def gemini_stream_generate_content(request: Request, model: str):
    """Gemini streamGenerateContent APIï¼ˆæµå¼ï¼‰"""
    from app.routes import handle_gemini_generate_content
    return await handle_gemini_generate_content(request, model, is_streaming=True, is_beta=False)


@app.post("/v1beta/models/{model}:generateContent")
async def gemini_beta_generate_content(request: Request, model: str):
    """Gemini beta generateContent APIï¼ˆéæµå¼ï¼‰"""
    from app.routes import handle_gemini_generate_content
    return await handle_gemini_generate_content(request, model, is_streaming=False, is_beta=True)


@app.post("/v1beta/models/{model}:streamGenerateContent")
async def gemini_beta_stream_generate_content(request: Request, model: str):
    """Gemini beta streamGenerateContent APIï¼ˆæµå¼ï¼‰"""
    from app.routes import handle_gemini_generate_content
    return await handle_gemini_generate_content(request, model, is_streaming=True, is_beta=True)


# ==================== Claude/Anthropic API ====================

@app.post("/v1/messages")
async def claude_messages(request: Request):
    """Claude Messages API"""
    from app.routes import handle_claude_messages
    return await handle_claude_messages(request)
